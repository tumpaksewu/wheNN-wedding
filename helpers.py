import os
import subprocess
import requests
import json
import numpy as np
import pickle
from collections import defaultdict
from PIL import Image
from pathlib import Path
from tqdm import tqdm
from typing import Generator

import torch
from torchvision import transforms

import hdbscan
import hnswlib
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA

from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.units import cm
import textwrap


# CLIP FUNCTIONS ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# TODO - evaluate and revert if needed
preprocess = transforms.Compose(
    [
        transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.48145466, 0.4578275, 0.40821073],
            std=[0.26862954, 0.26130258, 0.27577711],
        ),
    ]
)
# preprocess = transforms.Compose(
#     [
#         transforms.Resize(
#             (224, 224), interpolation=transforms.InterpolationMode.BICUBIC
#         ),
#         transforms.ToTensor(),
#         transforms.Normalize((0.4815, 0.4578, 0.4082), (0.2686, 0.2613, 0.2758)),
#     ]
# )


# Function to load and preprocess image
def load_image(img_path):
    image = Image.open(img_path).convert("RGB")
    return preprocess(image)


# Load images from folder and encode them
def get_image_embeddings(folder_path, device, clip_model):
    folder = Path(folder_path)
    image_paths = (
        list(folder.glob("*.jpg"))
        + list(folder.glob("*.png"))
        + list(folder.glob("*.jpeg"))
    )

    all_embeddings = []
    for img_path in tqdm(image_paths):
        image_tensor = load_image(img_path).unsqueeze(0).to(device)
        with torch.no_grad():
            embedding = clip_model.get_image_features(image_tensor)
            embedding = embedding / embedding.norm(dim=-1, keepdim=True)
            all_embeddings.append(embedding.cpu())

    return image_paths, torch.cat(all_embeddings, dim=0)


# Encode text query
def get_text_embedding(text, device, clip_model, processor):
    inputs = processor(text=[text], return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        embedding = clip_model.get_text_features(**inputs)
        embedding = embedding / embedding.norm(dim=-1, keepdim=True)
    return embedding.cpu()


# VECTOR-DB FUNCTIONS ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# Build hnswlib index + store metadata
# TODO - clean up path logic
def build_hnsw_index(
    embeddings_tensor,
    image_paths,
    index_path="./db/hnsw_index.bin",
    metadata_path="./db/hnsw_metadata.pkl",
):
    if not os.path.exists("./db"):
        os.makedirs("./db")

    # Convert to numpy array
    embeddings = embeddings_tensor.numpy().astype(np.float32)
    dim = embeddings.shape[1]
    num_elements = embeddings.shape[0]

    # Initialize Hnswlib index
    p = hnswlib.Index(space="cosine", dim=dim)
    p.init_index(max_elements=num_elements, ef_construction=200, M=16)

    # Add embeddings
    p.add_items(embeddings, ids=list(range(num_elements)))

    # Set ef for runtime search
    p.set_ef(50)

    # Save index
    p.save_index(index_path)

    # Format metadata
    video_filenames = []
    timestamps = []
    for posix_path in image_paths:
        filename = posix_path.name
        video_name, frame_number_str = filename.split("+")
        video_name = os.path.basename(video_name)
        frame_number = int(os.path.splitext(frame_number_str)[0])
        timestamp = frame_number * 3  # 3 sec/frame assumption
        video_filenames.append(video_name)
        timestamps.append(timestamp)

    # Save metadata
    metadata = {
        i: [str(image_paths[i]), video_filenames[i], timestamps[i]]
        for i in range(len(image_paths))
    }
    with open(metadata_path, "wb") as f:
        pickle.dump(metadata, f)

    print(f"Index and metadata saved: {index_path}, {metadata_path}")
    return p, metadata


# Load hnswlib index + metadata
def load_hnsw_index(
    index_path="./db/hnsw_index.bin", metadata_path="./db/hnsw_metadata.pkl", dim=512
):
    if not os.path.isfile(index_path):
        return None, None
    p = hnswlib.Index(space="cosine", dim=dim)
    p.load_index(index_path)

    with open(metadata_path, "rb") as f:
        metadata = pickle.load(f)

    return p, metadata


# Retrieve top-K using prebuilt hnswlib index + metadata file
def retrieve_top_k(query, hnsw_index, metadata, device, clip_model, processor, k=3):
    text_emb = get_text_embedding(query, device, clip_model, processor)
    text_emb_np = text_emb.numpy().astype("float32")

    labels, distances = hnsw_index.knn_query(text_emb_np, k=k)
    top_paths = [metadata[i][0] for i in labels[0]]
    video_filenames = [metadata[i][1] for i in labels[0]]
    timestamps = [metadata[i][2] for i in labels[0]]
    top_scores = [1 - d for d in distances[0]]

    return top_paths, top_scores, video_filenames, timestamps


# FFMPEG FUNCTIONS ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def extract_frames(
    video_path: str, output_dir: str, fps: float = 1 / 3
) -> Generator[str, None, None]:
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    video_name = os.path.basename(video_path)
    output_template = os.path.join(output_dir, f"{video_name}+%d.jpg")

    command = [
        "ffmpeg",
        "-i",
        video_path,
        "-ss",
        "3",
        "-vf",
        f"scale=480:-2,fps={fps}",
        output_template,
    ]

    process = subprocess.Popen(
        command,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True,
        bufsize=1,
    )

    for line in process.stdout:
        yield f"[ffmpeg] {line.strip()}"

    process.wait()


# TRANSLATION FUNCTIONS ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def ru_to_en(text: str, device, tr_model, tokenizer) -> str:
    if not text or not isinstance(text, str):
        return ""
    # Tokenizing
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=False,  # –û—Ç–∫–ª—é—á–∞–µ–º —Ç—Ä–µ–Ω–∫–∞—Ü–∏—é ‚Äî —á—Ç–æ–±—ã –Ω–µ –æ–±—Ä–µ–∑–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    ).to(device)
    # Translation generation
    translated_tokens = tr_model.generate(
        **inputs,
        max_length=512,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –≤—ã–≤–æ–¥–∞
        num_beams=8,  # –ë–æ–ª—å—à–µ –±–∏–º–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        length_penalty=1.0,  # –ë–æ–ª–µ–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ –¥–ª–∏–Ω–µ
        early_stopping=False,  # –ù–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω–æ
        no_repeat_ngram_size=2,  # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
    )
    # Decoding
    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)
    return translated_text


# CLUSTERING FUNCTIONS ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def cluster_embeddings(
    embeddings, n_components=64, min_cluster_size_ratio=0.015, min_samples=1
):
    # Step 1: Convert & reduce
    embeddings = normalize(embeddings)
    pca = PCA(n_components=n_components)
    reduced = pca.fit_transform(embeddings)

    # Step 2: Use dynamic min_cluster_size based on % of video length
    min_cluster_size = max(
        2, int(len(reduced) * min_cluster_size_ratio)
    )  # e.g., 1% of total frames

    # Step 3: Cluster with cosine metric
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size, min_samples=min_samples, metric="euclidean"
    )  # works on PCA-reduced vectors
    cluster_labels = clusterer.fit_predict(reduced)

    return cluster_labels


# Image grouping by cluster
def group_by_cluster(image_paths, cluster_labels, include_noise=True):
    clusters = defaultdict(list)
    for img_path, label in zip(image_paths, cluster_labels):
        if include_noise or label != -1:
            clusters[label].append(img_path)
    return clusters


# Get centroid images for each cluster
def get_centroid_images(clusters, embeddings):
    centroids = {}
    for cluster_id, indices in clusters.items():
        cluster_embeds = embeddings[indices]
        centroid = np.mean(cluster_embeds, axis=0)
        distances = np.linalg.norm(cluster_embeds - centroid, axis=1)
        min_idx = indices[np.argmin(distances)]
        centroids[cluster_id] = min_idx
    return centroids


# Run BLIP2 on centroid images to get captions
def describe_image(image_path, device, blip_model, blip_processor):
    image = Image.open(image_path).convert("RGB")
    inputs = blip_processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        output = blip_model.generate(**inputs, max_new_tokens=50)
    caption = blip_processor.tokenizer.decode(output[0], skip_special_tokens=True)
    return caption


# LLM FUNCTIONS ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def request_scenarios(centroid_captions, api_key, model, prompt=None):
    scenes = [
        {
            "description": item["caption"].strip("\n"),
            "image_path": item["image_path"],
            "video_filename": item["video_filename"],
            "timestamp": item["timestamp"],
        }
        for item in centroid_captions
    ]

    if not prompt:
        prompt = f"""
        –¢—ã –æ–ø—ã—Ç–Ω—ã–π –≤–∏–¥–µ–æ–º–æ–Ω—Ç–∞–∂—ë—Ä, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ —Å–≤–∞–¥–µ–±–Ω—ã—Ö –≤–∏–¥–µ–æ.  
        –¢–µ–±–µ –Ω—É–∂–Ω–æ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ª–æ–≥–∏—á–µ—Å–∫—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ü–µ–Ω (–ø–æ–¥ –∫–ª—é—á–∞–º–∏ description):

        {json.dumps(scenes, indent=2, ensure_ascii=False)}

        –°–¥–µ–ª–∞–π —Å–ª–µ–¥—É—é—â–µ–µ:

        1. –†–∞—Å—Å—Ç–∞–≤—å —Å—Ü–µ–Ω—ã –≤ —Ç–æ–º –ø–æ—Ä—è–¥–∫–µ, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –ø–ª–∞–≤–Ω—É—é –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é.
        2. –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ü–µ–Ω—ã –Ω–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: –ø–æ—á–µ–º—É –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –∏–¥—Ç–∏ –∏–º–µ–Ω–Ω–æ –∑–¥–µ—Å—å.
        3. –í –∫–æ–Ω—Ü–µ –æ–±—ä—è—Å–Ω–∏ –æ–±—â—É—é –ª–æ–≥–∏–∫—É –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è –∏ –µ–≥–æ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –¥—É–≥–∏.
        4. –¢–∞–∫–∂–µ, –Ω–µ –∑–∞–±—É–¥—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –æ–±—Ä–∞—Ç–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∏ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–µ (image_path, video_filename, timestamp). –ü—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–π—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–∏–∂–µ.

        –û—Ç–ø—Ä–∞–≤—å –æ—Ç–≤–µ—Ç —á–µ—Ç–∫–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –≤–∞–ª–∏–¥–Ω–æ–≥–æ JSON –≤ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ:
        {{
        "scenes": [
            {{
            "description": "–æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã",
            "justification": "–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ",
            "image_path": "–ø—É—Ç—å_–∫_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é",
            "video_filename": "–Ω–∞–∑–≤–∞–Ω–∏–µ_–≤–∏–¥–µ–æ—Ñ–∞–π–ª–∞",
            "timestamp": "–≤—Ä–µ–º–µ–Ω–Ω–æ–π_–∫–æ–¥",
            }},
        ],
        "overall_logic": "–û–±—â–∞—è –ª–æ–≥–∏–∫–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è"
        }}
        –ö—Ä–æ–º–µ —ç—Ç–æ–≥–æ JSON –≤ —Ç–≤–æ–µ–º –æ—Ç–≤–µ—Ç–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–∏—á–µ–≥–æ.
        –§–æ—Ä–º–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ø–æ–Ω—è—Ç–Ω–æ, –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤ —Ç–∏–ø–∞ "currentIndex", "kola!" –∏ –¥—Ä—É–≥–∏—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤.
        """

    response = requests.post(
        url="https://openrouter.ai/api/v1/chat/completions",
        headers={"Authorization": api_key, "Content-Type": "application/json"},
        data=json.dumps(
            {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
            }
        ),
    )
    return response


def decode_response(raw_content):
    try:
        data = json.loads(raw_content)
    except json.JSONDecodeError:
        # Try to extract the JSON manually in case there's surrounding text
        import re

        match = re.search(r"\{.*\}", raw_content, re.DOTALL)
        if match:
            json_str = match.group(0)
            data = json.loads(json_str)
        else:
            raise ValueError("No JSON object found in model response")
    return data


# PDF FUNCTIONS ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def draw_wrapped_text(c, text, x, y, max_width, font_size=12, leading=14):
    c.setFont("DejaVu", font_size)
    lines = textwrap.wrap(text, width=int(max_width / (font_size * 0.45)))
    for line in lines:
        c.drawString(x, y, line)
        y -= leading
    return y


# TODO - make a better pdf
def prepare_pdf(parsed_data, output_pdf="wedding_scenario_reportlab.pdf"):
    # === Register font with Cyrillic support ===
    pdfmetrics.registerFont(TTFont("DejaVu", "DejaVuSansCondensed.ttf"))

    # === Setup PDF ===
    c = canvas.Canvas(output_pdf, pagesize=A4)
    c.setFont("DejaVu", 14)
    width, height = A4
    margin = 2 * cm
    y = height - margin
    max_width = width - 2 * margin

    # === Title ===
    c.setFont("DejaVu", 18)
    c.drawCentredString(width / 2, y, "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ü–µ–Ω –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ")
    y -= 2 * cm

    # === Draw scenes ===
    for i, scene in enumerate(parsed_data["scenes"], 1):
        description = scene["description"]
        justification = scene["justification"]
        image_path = scene["image_path"]
        video_filename = scene["video_filename"]
        timestamp = scene["timestamp"]

        c.setFont("DejaVu", 14)
        y = draw_wrapped_text(
            c,
            f"–°—Ü–µ–Ω–∞ {i}: {description}",
            margin,
            y,
            max_width=max_width,
            font_size=13,
            leading=15,
        )
        y = draw_wrapped_text(
            c,
            f"–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {justification}",
            margin,
            y - 5,
            max_width=max_width,
            font_size=11,
            leading=13,
        )
        y = draw_wrapped_text(
            c,
            f"–î–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞: {image_path}, {video_filename}, {timestamp}",
            margin,
            y - 5,
            max_width=max_width,
            font_size=11,
            leading=13,
        )

        # Insert image if available
        if os.path.isfile(image_path):
            try:
                img = ImageReader(image_path)
                iw, ih = img.getSize()
                aspect = ih / iw
                img_width = (width - 2 * margin) / 2
                img_height = (img_width * aspect) / 2
                if y - img_height < margin:
                    c.showPage()
                    c.setFont("DejaVu", 14)
                    y = height - margin
                c.drawImage(
                    img, margin, y - img_height, width=img_width, height=img_height
                )
                y -= img_height + 1 * cm
            except Exception as e:
                y = draw_wrapped_text(
                    c,
                    f"[Image failed to load: {e}]",
                    margin,
                    y - 10,
                    max_width=max_width,
                )
        else:
            y = draw_wrapped_text(
                c, "[‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ]", margin, y - 10, max_width=max_width
            )

        if y < 5 * cm:
            c.showPage()
            c.setFont("DejaVu", 14)
            y = height - margin

    # === Draw overall logic at the end ===
    c.showPage()
    y = height - margin
    c.setFont("DejaVu", 16)
    c.drawCentredString(width / 2, y, "üé¨ –û–±—â–∞—è –ª–æ–≥–∏–∫–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è")
    y -= 2 * cm
    y = draw_wrapped_text(
        c,
        parsed_data["overall_logic"],
        margin,
        y,
        max_width=max_width,
        font_size=12,
        leading=15,
    )
    return c
